---
title: "EP10-respuesta-equipo-2"
output: html_document
---

```{r setup, include=FALSE}
library(ggpubr)
library(dplyr)
library(car)
```

```{r}
data <- read.csv2("EP09 Datos.csv", header = TRUE)
```

### Datos
```{r}
mujeres <- data[data$Gender == 0,]
mujeres$IMC <- mujeres$Weight / (mujeres$Height*0.01)^2

#Se define la variable dicotómica EN (estado nutricional) la cual depende del valor de IMC de cada persona.
# Sobrepreso = 1
# No sobrepreso = 0
mujeres$EN <- ifelse(mujeres$IMC >= 25.0, 1, 0)
```

##### 1. Definir la semilla a utilizar, que corresponde a los últimos cuatro dígitos del RUN (sin considerar el dígito verificador) del integrante de mayor edad del equipo.

Siguiendo las instrucciones indicadas, la semilla utilizada será 7704

```{r}
set.seed(7704)
```

##### 2. Seleccionar una muestra de 90 mujeres (si la semilla es un número par) o 90 hombres (si la semilla es impar), asegurando que la mitad tenga estado nutricional “sobrepeso” y la otra mitad “no sobrepeso” en cada caso. Dividir esta muestra en dos conjuntos: los datos de 60 personas (30 con EN “sobrepeso”) para utilizar en la construcción de los modelos y 30 personas (15 con EN “sobrepeso”) para poder evaluarlos.

Debido a que la semilla (7704) es par se seleccionará una muestra de 90 mujeres.

```{r}
# Muestra con sobre peso y sin sobrepeso
m_s <- mujeres[mujeres$EN == 1,]
m_ns <- mujeres[mujeres$EN == 0,]

# Buscamos que tanto la cantidad de mujeres con sobrepreso como las sin sobrepreso sean ambas 45
muestra_ns <- sample_n(m_ns, 45)
muestra_s <- sample_n(m_s, 45)

# Creamos los dataframes de entrenamiento con 30 mujeres con sobrepeso y 30 sin sobrepeso
train_s <- head(muestra_s, 30)
train_ns <- head(muestra_ns, 30)
train <- rbind(train_s, train_ns) #Se unen ambas muestras
train <- sample_n(train, 60, replace = FALSE) #Se realiza esto para evitar que la muestra esté ordenada

# Creamos el dataframe con que se evaluará el modelo
test_s <- tail(muestra_s, 15)
test_ns <- tail(muestra_ns, 15)
test <- rbind(test_s, test_ns) #Se unen ambas muestras
test <- sample_n(test, 30, replace = FALSE) #Se realiza esto para evitar que la muestra esté ordenada
```


##### 3. Recordar las ocho posibles variables predictoras seleccionadas de forma aleatoria en el ejercicio anterior.

Utilizando un generador aleatorio de números se han seleccionado las siguientes 8 posibles variables predictoras:

Chest.depth  
Shoulder.Girth  
Bitrochanteric.diameter  
Navel.Girth  
Hip.Girth  
Knee.Girth  
Ankles.diameter  
Knees.diameter  


### Modelo de regresión logística simple (RlogS)
##### 4. Seleccionar, de las otras variables, una que el equipo considere que podría ser útil para predecir la variable Peso, justificando bien esta selección.
 
Creemos que Waist.Girth (grosor a la altura de la cintura) podría ser útil para predecir la variable peso debido a que la mayor parte de la grasa corporal se acumula en esa zona, además algunos artículos relacionados con medicina afirman la cintura como un área en la cual se acumula gran parte de la grasa corporal [1] y [2].

##### 5. Usando el entorno R y paquetes estándares, construir un modelo de regresión logística con el predictor seleccionado en el paso anterior y utilizando de la muestra obtenida.

```{r}
modelo <- glm(EN ~ Waist.Girth, family = binomial(link = "logit"), data = train)
print(summary(modelo))
```

### Selección de variables y Modelo de regresión logística multiple (RlogM)
##### 6. Usando herramientas estándares para la exploración de modelos del entorno R, buscar entre dos y cinco predictores de entre las variables seleccionadas al azar, recordadas en el punto 3, para agregar al modelo obtenido en el paso 5.

Añadimos predictores uno por uno y observamos el cambio en el criterio de información de Akaike (AIC).

```{r}
# Exploración de modelos con selección hacia adelante
forward_model <- add1(modelo, scope = ~ Waist.Girth + Chest.depth + Shoulder.Girth + 
                        Bitrochanteric.diameter + Navel.Girth + Hip.Girth + 
                        Knee.Girth + Ankles.diameter + Knees.diameter, test = "Chisq")
print(forward_model)
```

Agregamos Hip.Girth debido a que es la variable con menor AIC = 25.92 al agregarla al modelo base.
```{r}
modeloMultiple <- update(modelo, . ~ . + Hip.Girth)
print(summary(modeloMultiple))
```

Ahora evaluamos el AIC si agregaramos las variables restantes.
```{r}
forward_model <- add1(modelo, scope = ~ Waist.Girth + Chest.depth + Shoulder.Girth + 
                        Bitrochanteric.diameter + Navel.Girth + Hip.Girth + 
                        Knee.Girth + Ankles.diameter + Knees.diameter, test = "Chisq")
print(forward_model)
```

Dado que necesitamos otra variable para predecir el estado nutricional EN según lo solicitado en el enunciado del ejercicio, agregamos la variable con menor AIC, en este caso, Ankles.diameter con AIC = 26.717 aunque no haya mejorado el modelo.
```{r}
modeloMultiple <- update(modeloMultiple, . ~ . + Ankles.diameter)
print(summary(modeloMultiple))
```


### Confiabilidad de los modelos
##### 7. Evaluar la confiabilidad de los modelos (i.e. que tengan un buen nivel de ajuste y son generalizables) y “arreglarlos” en caso de que tengan algún problema.

##### Modelo de regresión logística

```{r}
# Comprobar la independencia de los residuos.
cat ("Prueba de Durbin - Watson para autocorrelaciones")
cat ("entre errores :\n ")
print(durbinWatsonTest(modelo))

# Comprobar normalidad de los residuos
cat ("\nPrueba de normalidad para los residuos:\n" )
print(shapiro.test(modelo$residuals))

# Comprobar homocedasticidad de los residuos.
# Obtener los residuos del modelo
residuos <- residuals(modelo, type = "response")

# Gráfico de dispersión de residuos frente a los valores ajustados
plot(predict(modelo, type = "response"), residuos, ylab = "Residuos", xlab = "Valores ajustados", main = "Homocedasticidad")

# Opcional: añadir una línea horizontal en y = 0 para facilitar la visualización
abline(h = 0, col = "red")

# Crear gráficos adicionales como un gráfico quantil-quantil (Q-Q plot) de los residuos
qqnorm(residuos)
qqline(residuos)
```

##### Modelo de regresión multiple

```{r}
# Comprobar la independencia de los residuos.
cat ("Prueba de Durbin - Watson para autocorrelaciones")
cat ("entre errores :\n ")
print(durbinWatsonTest(modeloMultiple))

# Comprobar normalidad de los residuos
cat ("\nPrueba de normalidad para los residuos:\n" )
print(shapiro.test(modeloMultiple$residuals))

# Comprobar homocedasticidad de los residuos.
# Obtener los residuos del modelo
residuosMultiple <- residuals(modeloMultiple, type = "response")

# Gráfico de dispersión de residuos frente a los valores ajustados
plot(predict(modeloMultiple, type = "response"), residuosMultiple, ylab = "Residuos", xlab = "Valores ajustados", main = "Homocedasticidad")

# Opcional: añadir una línea horizontal en y = 0 para facilitar la visualización
abline(h = 0, col = "red")

# Crear gráficos adicionales como un gráfico quantil-quantil (Q-Q plot) de los residuos
qqnorm(residuosMultiple)
qqline(residuosMultiple)

# Comprobar la multicolinealidad entre las variables
vifsMultiple <- vif(modeloMultiple)
vifsMultiple
print(1/vifsMultiple)
print(mean(vifsMultiple))
```

Al verificar las condiciones, en ambos modelos se falla en rechazar las hipótesis nulas de las pruebas realizadas, con excepción de la comprobación de la normalidad de los residuos, sin embargo esto no es un inconveniente mayor debido a que este tipo de modelos no asume normalidad en los residuos como lo haría un modelo de regresión lineal.

### Calidad predictiva de los modelos

##### 8. Usando código estándar, evaluar el poder predictivo de los modelos con los datos de las 40 personas que no se incluyeron en su construcción en términos de sensibilidad y especificidad.

##### Evaluacion modelo 1
```{r}
umbral_modelo1 <- 0.5

probabilidades_modelo1 <- predict(modelo, test, type = "response")
predicciones_modelo1 <- sapply(probabilidades_modelo1, function(p) ifelse(p >= umbral_modelo1, "1" , "0"))
predicciones_modelo1 <- factor(predicciones_modelo1)

# Organizar los valores de probabilidad y etiquetas
orden_modelo1 <- order(probabilidades_modelo1, decreasing = TRUE)
probabilidades_ordenadas_modelo1 <- probabilidades_modelo1[orden_modelo1]
etiquetas_reales_modelo1 <- as.numeric(test$EN)[orden_modelo1]
predicciones_factor_modelo1 = as.factor(predicciones_modelo1)
etiquetas_factor_modelo1 = as.factor(etiquetas_reales_modelo1)
# Configurar vectores para las tasas de positivos verdaderos y falsos positivos.
tasa_vp_modelo1 <- c(0)
tasa_fp_modelo1 <- c(0)

# Determinar los valores de Positivos Verdaderos (VP), Negativos Verdaderos (VN), Falsos Positivos (FP) y Falsos Negativos (FN).
VP_modelo1 <- sum(predicciones_modelo1 == "1" & etiquetas_reales_modelo1 == 1)
VN_modelo1 <- sum(predicciones_modelo1 == "0" & etiquetas_reales_modelo1 == 0)
FP_modelo1 <- sum(predicciones_modelo1 == "1" & etiquetas_reales_modelo1 == 0)
FN_modelo1 <- sum(predicciones_modelo1 == "0" & etiquetas_reales_modelo1 == 1)
# Generar la matriz de confusión.
confusion_matrix_modelo1 <- matrix(c(VN_modelo1, FN_modelo1, FP_modelo1, VP_modelo1), nrow = 2, byrow = TRUE, dimnames = list("Predicción" = c("0", "1"), "Etiqueta Real" = c("0", "1")))
print(confusion_matrix_modelo1)

# Determinar las tasas de positivos verdaderos y falsos positivos.
for (i in 1:length(probabilidades_ordenadas_modelo1)) {
  predicciones_modelo1 <- ifelse(probabilidades_ordenadas_modelo1 >= probabilidades_ordenadas_modelo1[i], "1", "0")
  vp_modelo1 <- sum(predicciones_modelo1 == "1" & etiquetas_reales_modelo1 == 1)
  fp_modelo1 <- sum(predicciones_modelo1 == "1" & etiquetas_reales_modelo1 == 0)
  tasa_vp_modelo1 <- c(tasa_vp_modelo1, vp_modelo1/sum(etiquetas_reales_modelo1 == 1))
  tasa_fp_modelo1 <- c(tasa_fp_modelo1, fp_modelo1/sum(etiquetas_reales_modelo1 == 0))
}

# Representar gráficamente la curva ROC.
plot(tasa_fp_modelo1, tasa_vp_modelo1, type = "l", main = "Curva ROC Modelo 1", xlab = "Especificidad", ylab = "Sensibilidad")
lines(c(0,1), c(0,1), col = "blue")

# Mostrar el área bajo la curva ROC (AUC).
auc_modelo1 <- sum(diff(tasa_fp_modelo1) * (tasa_vp_modelo1[-length(tasa_vp_modelo1)] + tasa_vp_modelo1[-1])) / 2
cat("Área bajo la curva ROC Modelo 1: ", round(auc_modelo1, 2), "\n")
```
Se puede observar que la curva se aleja considerablemente de la diagonal, lo que sugiere un buen modelo para predecir, el área bajo la curva es bastante cercano a 1 (0.93).

##### Evaluacion modelo 2
```{r}
umbral_modelo2 <- 0.5

probabilidades_modelo2 <- predict(modeloMultiple, test, type = "response")
predicciones_modelo2 <- sapply(probabilidades_modelo2, function(p) ifelse(p >= umbral_modelo2, "1" , "0"))
predicciones_modelo2 <- factor(predicciones_modelo2)

# Organizar los valores de probabilidad y etiquetas
orden_modelo2 <- order(probabilidades_modelo2, decreasing = TRUE)
probabilidades_ordenadas_modelo2 <- probabilidades_modelo2[orden_modelo2]
etiquetas_reales_modelo2 <- as.numeric(test$EN)[orden_modelo2]
predicciones_factor_modelo2 = as.factor(predicciones_modelo2)
etiquetas_factor_modelo2 = as.factor(etiquetas_reales_modelo2)
# Configurar vectores para las tasas de positivos verdaderos y falsos positivos.
tasa_vp_modelo2 <- c(0)
tasa_fp_modelo2 <- c(0)

# Determinar los valores de Positivos Verdaderos (VP), Negativos Verdaderos (VN), Falsos Positivos (FP) y Falsos Negativos (FN).
VP_modelo2 <- sum(predicciones_modelo2 == "1" & etiquetas_reales_modelo2 == 1)
VN_modelo2 <- sum(predicciones_modelo2 == "0" & etiquetas_reales_modelo2 == 0)
FP_modelo2 <- sum(predicciones_modelo2 == "1" & etiquetas_reales_modelo2 == 0)
FN_modelo2 <- sum(predicciones_modelo2 == "0" & etiquetas_reales_modelo2 == 1)
# Generar la matriz de confusión.
confusion_matrix_modelo2 <- matrix(c(VN_modelo2, FN_modelo2, FP_modelo2, VP_modelo2), nrow = 2, byrow = TRUE, dimnames = list("Predicción" = c("0", "1"), "Etiqueta Real" = c("0", "1")))
print(confusion_matrix_modelo2)

# Determinar las tasas de positivos verdaderos y falsos positivos.
for (i in 1:length(probabilidades_ordenadas_modelo2)) {
  predicciones_modelo2 <- ifelse(probabilidades_ordenadas_modelo2 >= probabilidades_ordenadas_modelo2[i], "1", "0")
  vp_modelo2 <- sum(predicciones_modelo2 == "1" & etiquetas_reales_modelo2 == 1)
  fp_modelo2 <- sum(predicciones_modelo2 == "1" & etiquetas_reales_modelo2 == 0)
  tasa_vp_modelo2 <- c(tasa_vp_modelo2, vp_modelo2/sum(etiquetas_reales_modelo2 == 1))
  tasa_fp_modelo2 <- c(tasa_fp_modelo2, fp_modelo2/sum(etiquetas_reales_modelo2 == 0))
}

# Representar gráficamente la curva ROC.
plot(tasa_fp_modelo2, tasa_vp_modelo2, type = "l", main = "Curva ROC Modelo 2", xlab = "Especificidad", ylab = "Sensibilidad")
lines(c(0,1), c(0,1), col = "blue")

# Mostrar el área bajo la curva ROC (AUC).
auc_modelo2 <- sum(diff(tasa_fp_modelo2) * (tasa_vp_modelo2[-length(tasa_vp_modelo2)] + tasa_vp_modelo2[-1])) / 2
cat("Área bajo la curva ROC Modelo 2: ", round(auc_modelo2, 2), "\n")
```
Se puede observar que la curva se aleja considerablemente de la diagonal, lo que sugiere un buen modelo, además el área bajo la curva es de 0.94, siendo mayor al área bajo la curva en el modelo 1, por lo que se recomienda el modelo 2 sobre el modelo 1.


Referencias:

[1] Corpo, & Corpo. (2021, 28 enero). Descubre dónde acumulas más grasa segun la forma de tu cuerpo - Corpore Clinic. Corpore Clinic - Cirugia Plastica y estética. https://corporeclinic.cl/descubre-donde-acumulas-mas-grasa-segun-la-forma-de-tu-cuerpo/

[2] Ponce, I. G. (2022, 7 diciembre). ¿En qué zonas del cuerpo es más peligroso que haya grasa? CuidatePlus. https://cuidateplus.marca.com/bienestar/2021/04/08/zonas-cuerpo-mas-peligroso-haya-grasa-177590.html#:~:text=La%20grasa%20corporal%20se%20reparte,%2C%20el%20h%C3%ADgado%2C%20los%20ri%C3%B1ones%E2%80%A6

********************

```{r}
library(car)
library(ggpubr)
library(lmtest)
library(tidyverse)
```
################################################################################
# Vamos a construir un modelo de regresión logística para predecir la variable
# EN, de acuerdo con las siguientes instrucciones:
# 1. Definir la semilla a utilizar, que corresponde a los últimos cuatro dígitos
#    del RUN (sin considerar el dígito verificador) del integrante de mayor edad
#    del equipo.
# 2. Seleccionar una muestra de 90 mujeres (si la semilla es un número par) o
#    90 hombres (si la semilla es impar), asegurando que la mitad tenga estado
#    nutricional “sobrepeso” y la otra mitad “no sobrepeso”. Dividir esta
#    muestra en dos conjuntos: los datos de 60 personas (30 con EN “sobrepeso”)
#    para utilizar en la construcción de los modelos y 30 personas (15 con EN
#    “sobrepeso”) para poder evaluarlos.
################################################################################
```{r}
# Fijamos la carpeta de trabajo
setwd("~/Downloads")

# Cargamos los datos
datos <- read.csv2("Datos Heinz et al 2003.csv")
datos.ext <- datos %>% mutate(IMC = Weight / (Height / 100)**2) %>%
  mutate(EN = ifelse(IMC < 25, "no sobrepeso", "sobrepeso"))
datos.ext[["Gender"]] <- factor(datos.ext[["Gender"]])
datos.ext[["EN"]] <- factor(datos.ext[["EN"]])
datos.ext[["Id"]] <- 1:nrow(datos.ext) # Para revisar independencia de las muestras

# Fijamos la semilla
set.seed(1111)

# Obtenemos la muestra
muestra.a <- datos.ext %>% filter(Gender == 1 & EN == "no sobrepeso") %>%
  sample_n(45, replace = FALSE)
muestra.b <- datos.ext %>% filter(Gender == 1 & EN == "sobrepeso") %>%
  sample_n(45, replace = FALSE)

i.train <- sample(1:45, 30)
muestra.train <- rbind(muestra.a[i.train, ], muestra.b[i.train, ])
muestra.test <- rbind(muestra.a[-i.train, ], muestra.b[-i.train, ])

# Verificamos que no cometimos algún error con las muestras
stopifnot(all(muestra.train$Id == unique(muestra.train$Id)))
stopifnot(all(muestra.test$Id == unique(muestra.test$Id)))
stopifnot(!any(muestra.train$Id %in% muestra.test))

# Vamos a desordenar la muestra para que no queden ordenados los grupos
muestra.train <- muestra.train[sample(1:nrow(muestra.train)), ]
muestra.test <- muestra.test[sample(1:nrow(muestra.test)), ]
```

################################################################################
# 3. Recordar las ocho posibles variables predictoras seleccionadas de forma
#    aleatoria en el ejercicio anterior.
################################################################################
```{r}
pred.pos.noms <-c("Knee.Girth", "Bicep.Girth", "Ankles.diameter", "Chest.depth",
                  "Shoulder.Girth", "Navel.Girth", "Hip.Girth", "Biiliac.diameter")
```
################################################################################
# 4. Seleccionar, de las otras variables, una que el equipo considere que podría
#    ser útil para predecir la clase EN, justificando bien esta selección.
# 5. Usando el entorno R y paquetes estándares, construir un modelo de
#    regresión logística con el predictor seleccionado en el paso anterior y
#    utilizando de la muestra obtenida.
################################################################################

# Vamos a elegir la variable peso como predictor para el modelo de RLogS, pues
# esta variable está fuertemente relacionada con el IMC, que a su vez se usa
# como indicador para determinar el sobrepeso.
```{r}
rlogs <- glm(EN ~ Weight, data = muestra.train,
             family = binomial(link = "logit"))

cat("\nModelo de regresión logística simple\n")
print(summary(rlogs))
```
################################################################################
# 6. Usando herramientas estándares para la exploración de modelos del entorno
#    R, buscar entre dos y cinco predictores de entre las variables
#    seleccionadas al azar, recordadas en el punto 3, para agregar al modelo
#    obtenido en el paso 5.
################################################################################

# Podemos buscar un predictor (desde los elegidos aleatoriamente como posibles)
# que nos ayude a mejorar el modelo de RLogS usando llamadas reiteradas a la
# función add1(), por ejemplo:
```{r}
cat("\n")
print(add1(rlogs, scope = pred.pos.noms))
```
# O podríamos crear un modelo máximo con todas los posibles predictores elegidos
# y usar llamadas reiteradas a la función drop1(), por ejemplo:
```{r}
fmla <- formula(paste(c(". ~ .", pred.pos.noms), collapse = " + "))
rlogm.max <- update(rlogs, fmla)
print(drop1(rlogm.max))
```
# O podemos hacer que R busque un modelo entre nuestro modelo de RLogS y el
# modelo máximo:
```{r}
print(rlogm <- step(rlogs, scope = list(lower = rlogs, upper = rlogm.max),
                    direction = "both"))
```
################################################################################
# 7. Evaluar la confiabilidad de los modelos (i.e. que tengan un buen nivel de
#    ajuste y son generalizables) y “arreglarlos” en caso de que tengan algún
#    problema.
################################################################################

# Evaluemos primero el modelo de RLogS
```{r}
# Primero, revisemos el ajuste
cat("\nModelo de RLogS encontrado:")
print(summary(rlogs))
```
# Podemos ver que utilizar el peso para predecir el estado nutricional, tal y
# como sospechábamos, conseguimos una importante reducción de "devianza", lo
# que nos indica que el modelo  consigue un buen ajuste.

# Ahora revisemos los residuos y estadísticas de influencia de los casos
```{r}
eval.rlogs <- data.frame(standardized.residuals = rstandard(rlogs))
eval.rlogs[["studentized.residuals"]] <-rstudent(rlogs)
eval.rlogs[["cooks.distance"]] <- cooks.distance(rlogs)
eval.rlogs[["dfbeta"]] <- dfbeta(rlogs)
eval.rlogs[["dffit"]] <- dffits(rlogs)
eval.rlogs[["leverage"]] <- hatvalues(rlogs)
eval.rlogs[["covariance.ratios"]] <- covratio(rlogs)

cat("\nInfluencia de los casos:\n")
cat("------------------------\n")
```
# 95% de los residuos estandarizados deberían estar entre −1.96 y +1.96, y 99%
# entre -2.58 y +2.58.
```{r}
sospechosos1 <- which(abs(eval.rlogs[["standardized.residuals"]]) > 1.96)
cat("- Residuos estandarizados fuera del 95% esperado: ")
print(sospechosos1)
```
# Observaciones con distancia de Cook mayor a uno.
```{r}
sospechosos2 <- which(eval.rlogs[["cooks.distance"]] > 1)
cat("- Residuos con distancia de Cook mayor que 1: ")
print(sospechosos2)
```
# Observaciones con apalancamiento superior al doble del apalancamiento
# promedio: (k + 1)/n.
```{r}
apalancamiento.promedio <- 2 / nrow(muestra.train)
sospechosos3 <- which(eval.rlogs[["leverage"]] > 2 * apalancamiento.promedio)

cat("- Residuos con apalancamiento fuera de rango (promedio = ",
    apalancamiento.promedio, "): ", sep = "")

print(sospechosos3)
```
# DFBeta debería ser < 1.
```{r}
sospechosos4 <- which(apply(eval.rlogs[["dfbeta"]] >= 1, 1, any))
names(sospechosos4) <- NULL
cat("- Residuos con DFBeta mayor que 1: ")
print(sospechosos4)
```
# Finalmente, los casos no deberían desviarse significativamente
# de los límites recomendados para la razón de covarianza:
# CVRi mayor que 1 + [3(k + 1)/n]
# CVRi menor que 1 – [3(k + 1)/n]
```{r}
CVRi.lower <- 1 - 3 * apalancamiento.promedio
CVRi.upper <- 1 + 3 * apalancamiento.promedio
sospechosos5 <- which(eval.rlogs[["covariance.ratios"]] < CVRi.lower |
                        eval.rlogs[["covariance.ratios"]] > CVRi.upper)
cat("- Residuos con razón de covarianza fuera de rango ([", CVRi.lower, ", ",
    CVRi.upper, "]): ", sep = "")

print(sospechosos5)

sospechosos <- c(sospechosos1, sospechosos2, sospechosos3, sospechosos4,
                 sospechosos5)

sospechosos <- sort(unique(sospechosos))
cat("\nResumen de observaciones sospechosas:\n")

print(round(eval.rlogs[sospechosos,
                     c("cooks.distance", "leverage", "covariance.ratios")],
            3))
```
# Si bien hay algunas observaciones que podrían considerarse atípicas, la
# distancia de Cook para todas ellas se aleja bastante de 1, por lo que no
# deberían ser causa de preocupación.

# Ahora verifiquemos el supuesto de linealidad de la relación
```{r}
xs1 <- data.frame(Logit = log(fitted(rlogs)/(1-fitted(rlogs))),
                  Weight = muestra.train[["Weight"]])
pxs1 <- ggscatter(data = xs1, x = "Logit", y = "Weight", conf.int = TRUE)
print(pxs1)
```
# Vemos que es perfectamente lineal !!

# Revisemos ahora los residuos y si cumplen las condiciones necesarias
```{r}
xs2 <- data.frame(Indice = 1:nrow(muestra.train),
                  Residuo.estandarizado = rstandard(rlogs))
pxs2 <- ggscatter(data = xs2, x = "Indice", y = "Residuo.estandarizado")
print(pxs2)
```
# Vemos que no aparece un patrón claro ni una variación consistente de la
# varianza, lo que podemos confirmar con pruebas auxiliares:
```{r}
print(shapiro.test(resid(rlogs)))
print(bptest(rlogs))
```
# Cumple bien con estos requisitos.

# Finalmente, revisamos que los residuos sean independientes

```{r}
cat("\nPrueba de Durbin y Watson:\n")
print(durbinWatsonTest(rlogs))
```
# Vemos que no hay razones para sospechar que los residuos no sean independientes

# Así, el modelo de RLogS parece tener un buen ajuste y cumplir bien con las
# condiciones para ser confiable.



# Veamos ahora el modelo de RLogM.

# Primero, revisemos el ajuste
```{r}
cat("\nModelo de RLogM encontrado:")
print(summary(rlogm))
```
# Podemos ver que el modelo consigue una importante reducción de la "devianza",
# indicando que se consigue un buen ajuste. Sin embargo, es preocupante que,
# ahora, la variable peso aparezca como poco relevante para el modelo. Debemos
# estar atentos a problemas de multicolinealidad. Evaluemos esta condición.
```{r}
cat("\nFactores de inflación de la varianza:\n")
print(vif(rlogm))
print(1 / vif(rlogm))
```
# Podemos notar que todos los factores de inflación de la varianza son lejanos
# al límite de 10 y ninguna tolerancia es menos a 0.2, lo que indicaría que no
# hay presencia de multicolinealidad.

# Ahora revisemos los residuos y estadísticas de influencia de los casos

```{r}
eval.rlogm <- data.frame(standardized.residuals = rstandard(rlogm))
eval.rlogm[["studentized.residuals"]] <-rstudent(rlogm)
eval.rlogm[["cooks.distance"]] <- cooks.distance(rlogm)
eval.rlogm[["dfbeta"]] <- dfbeta(rlogm)
eval.rlogm[["dffit"]] <- dffits(rlogm)
eval.rlogm[["leverage"]] <- hatvalues(rlogm)
eval.rlogm[["covariance.ratios"]] <- covratio(rlogm)

cat("\nInfluencia de los casos:\n")
cat("------------------------\n")

# 95% de los residuos estandarizados deberían estar entre −1.96 y +1.96, y 99%
# entre -2.58 y +2.58.
sospechosos1 <- which(abs(eval.rlogm[["standardized.residuals"]]) > 1.96)
cat("- Residuos estandarizados fuera del 95% esperado: ")
print(sospechosos1)

# Observaciones con distancia de Cook mayor a uno.
sospechosos2 <- which(eval.rlogm[["cooks.distance"]] > 1)
cat("- Residuos con distancia de Cook mayor que 1: ")
print(sospechosos2)

# Observaciones con apalancamiento superior al doble del apalancamiento
# promedio: (k + 1)/n.
apalancamiento.promedio <- 2 / nrow(muestra.train)
sospechosos3 <- which(eval.rlogm[["leverage"]] > 2 * apalancamiento.promedio)

cat("- Residuos con apalancamiento fuera de rango (promedio = ",
    apalancamiento.promedio, "): ", sep = "")

print(sospechosos3)

# DFBeta debería ser < 1.
sospechosos4 <- which(apply(eval.rlogm[["dfbeta"]] >= 1, 1, any))
names(sospechosos4) <- NULL
cat("- Residuos con DFBeta mayor que 1: ")
print(sospechosos4)

# Finalmente, los casos no deberían desviarse significativamente
# de los límites recomendados para la razón de covarianza:
# CVRi > 1 + [3(k + 1)/n]
# CVRi < 1 – [3(k + 1)/n]
CVRi.lower <- 1 - 3 * apalancamiento.promedio
CVRi.upper <- 1 + 3 * apalancamiento.promedio
sospechosos5 <- which(eval.rlogm[["covariance.ratios"]] < CVRi.lower |
                        eval.rlogm[["covariance.ratios"]] > CVRi.upper)
cat("- Residuos con razón de covarianza fuera de rango ([", CVRi.lower, ", ",
    CVRi.upper, "]): ", sep = "")

print(sospechosos5)

sospechosos <- c(sospechosos1, sospechosos2, sospechosos3, sospechosos4,
                 sospechosos5)

sospechosos <- sort(unique(sospechosos))
cat("\nResumen de observaciones sospechosas:\n")

print(round(eval.rlogm[sospechosos,
                       c("cooks.distance", "leverage", "covariance.ratios")],
            3))
```
# Vemos que, si bien hay muchos residuos con valores atípicas, ninguno muestra
# un valor preocupante en la distancia de Cook o de apalancamiento. Por lo que
# el modelo no parece sufrir de casos muy influyentes.

# Ahora verifiquemos el supuesto de linealidad de la relación
```{r}
xm1 <- data.frame(Logit = log(fitted(rlogs)/(1-fitted(rlogs))),
                   Weight = muestra.train[["Weight"]],
                   Hip.Girth = muestra.train[["Hip.Girth"]],
                   Bicep.Girth = muestra.train[["Bicep.Girth"]])
xm1.l <- pivot_longer(xm1, -Logit, names_to = "Predictor", values_to = "Valor")
pxm1 <- ggscatter(data = xm1.l, x = "Logit", y = "Valor", conf.int = TRUE) +
  geom_smooth(method = "loess") + 
  theme_bw() +
  facet_wrap(~ Predictor, scales = "free_y")
print(pxm1)
```
# Podemos ver que obviamente peso tiene una relación lineal perfecta (recordar
# que la usamos para definir la variable de salida EN) y las otras muestran un
# comportamiento mayoritariamente lineal con algunos valores atípicos.

# Revisemos ahora los residuos y si cumplen las condiciones necesarias

```{r}
xm2 <- data.frame(Indice = 1:nrow(muestra.train),
                  Residuo.estandarizado = rstandard(rlogm))
pxm2 <- ggscatter(data = xm2, x = "Indice", y = "Residuo.estandarizado")
print(pxm2)
```
# Vemos que, como en el modelo simple, no aparece un patrón claro ni una 
# variación consistente de la varianza, lo que podemos confirmar con pruebas
# auxiliares:
```{r}
print(shapiro.test(resid(rlogm)))
print(bptest(rlogm))
```
# Cumple bien con estos requisitos.

# Finalmente, revisamos que los residuos sean independientes
```{r}
cat("\nPrueba de Durbin y Watson:\n")
print(durbinWatsonTest(rlogm))
```
# Al igual que con el modelo simple, no hay motivos para rechazar la idea de que
# los residuos son independientes.

# Concluimos que el modelo de RLogM que hemos obtenido tiene un buen ajuste y
# cumple de forma satisfactoria con las condiciones para ser considerado confiable.


################################################################################
# 8. Usando código estándar, evaluar el poder predictivo de los modelos con los
#    datos de las 30 personas que no se incluyeron en su construcción en
#    términos de sensibilidad y especificidad.
################################################################################
```{r}
# Usaremos el umbral por defecto
umbral <- 0.5

# Primero calculemos el poder predictivo del modelo RLogS en los datos de 
# entrenamiento.
probs.trs <- fitted(rlogs)
preds.trs <- sapply(probs.trs,
                    function (p) ifelse (p >= umbral, "sobrepeso", "no sobrepeso"))
preds.trs <- factor(preds.trs, levels = levels(muestra.train[["EN"]]))
TP.trs <- sum(muestra.train[["EN"]] == "sobrepeso" & preds.trs == "sobrepeso")
FP.trs <- sum(muestra.train[["EN"]] == "no sobrepeso" & preds.trs == "sobrepeso")
TN.trs <- sum(muestra.train[["EN"]] == "no sobrepeso" & preds.trs == "no sobrepeso")
FN.trs <- sum(muestra.train[["EN"]] == "sobrepeso" & preds.trs == "no sobrepeso")
acc.trs <- (TP.trs + TN.trs) / (TP.trs + FP.trs + TN.trs + FN.trs)
sen.trs <- TP.trs / (TP.trs + FN.trs)
esp.trs <- TN.trs / (TN.trs + FP.trs)

# Ahora calculemos el poder predictivo del modelo RLogS en los datos de prueba 
probs.tes <- predict(rlogs, muestra.test, type = "response")
preds.tes <- sapply(probs.tes,
                    function (p) ifelse (p >= umbral, "sobrepeso", "no sobrepeso"))
preds.tes <- factor(preds.tes, levels = levels(muestra.test[["EN"]]))
TP.tes <- sum(muestra.test[["EN"]] == "sobrepeso" & preds.tes == "sobrepeso")
FP.tes <- sum(muestra.test[["EN"]] == "no sobrepeso" & preds.tes == "sobrepeso")
TN.tes <- sum(muestra.test[["EN"]] == "no sobrepeso" & preds.tes == "no sobrepeso")
FN.tes <- sum(muestra.test[["EN"]] == "sobrepeso" & preds.tes == "no sobrepeso")
acc.tes <- (TP.tes + TN.tes) / (TP.tes + FP.tes + TN.tes + FN.tes)
sen.tes <- TP.tes / (TP.tes + FN.tes)
esp.tes <- TN.tes / (TN.tes + FP.tes)

cat("\nRendimiento del modelo de RLogS:\n")
cat("    Exactitud entrenamiento:", sprintf("%.2f", acc.trs * 100), "\n")
cat("           Exactitud prueba:", sprintf("%.2f", acc.tes * 100), "\n")
cat(" Sensibilidad entrenamiento:", sprintf("%.2f", sen.trs * 100), "\n")
cat("        Sensibilidad prueba:", sprintf("%.2f", sen.tes * 100), "\n")
cat("Especificidad entrenamiento:", sprintf("%.2f", esp.trs * 100), "\n")
cat("       Especificidad prueba:", sprintf("%.2f", esp.tes * 100), "\n")
cat("\n")

# Podemos observar valores similares en el poder predictivo del modelo de
# RLogS en el conjunto de entrenamiento y en el conjunto de prueba. Esto
# es un indicador de que el modelo logra un buen nivel de aprendizaje. Así,
# debemos concluir que este modelo también es generalizable. 


# Ahora calculemos el poder predictivo del modelo RLogM en los datos de 
# entrenamiento.
probs.trm <- fitted(rlogm)
preds.trm <- sapply(probs.trm,
                    function (p) ifelse (p >= umbral, "sobrepeso", "no sobrepeso"))
preds.trm <- factor(preds.trm, levels = levels(muestra.train[["EN"]]))
TP.trm <- sum(muestra.train[["EN"]] == "sobrepeso" & preds.trm == "sobrepeso")
FP.trm <- sum(muestra.train[["EN"]] == "no sobrepeso" & preds.trm == "sobrepeso")
TN.trm <- sum(muestra.train[["EN"]] == "no sobrepeso" & preds.trm == "no sobrepeso")
FN.trm <- sum(muestra.train[["EN"]] == "sobrepeso" & preds.trm == "no sobrepeso")
acc.trm <- (TP.trm + TN.trm) / (TP.trm + FP.trm + TN.trm + FN.trm)
sen.trm <- TP.trm / (TP.trm + FN.trm)
esp.trm <- TN.trm / (TN.trm + FP.trm)

# Ahora calculemos el poder predictivo del modelo RLogS en los datos de prueba 
probs.tem <- predict(rlogm, muestra.test, type = "response")
preds.tem <- sapply(probs.tem,
                    function (p) ifelse (p >= umbral, "sobrepeso", "no sobrepeso"))
preds.tem <- factor(preds.tem, levels = levels(muestra.test[["EN"]]))
TP.tem <- sum(muestra.test[["EN"]] == "sobrepeso" & preds.tem == "sobrepeso")
FP.tem <- sum(muestra.test[["EN"]] == "no sobrepeso" & preds.tem == "sobrepeso")
TN.tem <- sum(muestra.test[["EN"]] == "no sobrepeso" & preds.tem == "no sobrepeso")
FN.tem <- sum(muestra.test[["EN"]] == "sobrepeso" & preds.tem == "no sobrepeso")
acc.tem <- (TP.tem + TN.tem) / (TP.tem + FP.tem + TN.tem + FN.tem)
sen.tem <- TP.tem / (TP.tem + FN.tem)
esp.tem <- TN.tem / (TN.tem + FP.tem)

cat("\nRendimiento del modelo de RLogM:\n")
cat("    Exactitud entrenamiento:", sprintf("%.2f", acc.trm * 100), "\n")
cat("           Exactitud prueba:", sprintf("%.2f", acc.tem * 100), "\n")
cat(" Sensibilidad entrenamiento:", sprintf("%.2f", sen.trm * 100), "\n")
cat("        Sensibilidad prueba:", sprintf("%.2f", sen.tem * 100), "\n")
cat("Especificidad entrenamiento:", sprintf("%.2f", esp.trm * 100), "\n")
cat("       Especificidad prueba:", sprintf("%.2f", esp.tem * 100), "\n")
cat("\n")
```
# Como en el caso del modelo simple, vemos que el poder predictivo del modelo de
# RLogM es similar en el conjunto de entrenamiento y en el conjunto de prueba,
# incluso parece un poco mejor en este último (aunque esto requiere de una prueba
# de McNemar para confirmarse). De esta forma, podemos concluir que este modelo
# también parece generalizable.